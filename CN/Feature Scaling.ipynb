{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Section will be for Feature Scaling , why used , where used etc = <br>\n",
    "we use this as a part of data preprocessing , before applying any algo , model or regressonr , classifier etc <br>\n",
    "we want all the features to lie in an unifrom range (usually waht happens is that the feature with a range of higher values overpowers the features with lower valued ranges for most algorithms , to remove this feature scaling is important)\n",
    "<br> <br>\n",
    "important example - find nearest neighbour - housing are sqft(range-1000,2000) , noofbedrooms(range-1,4)<br>\n",
    "if we dont scale - and then calculate - if training points are -<br>\n",
    "1 - 1200     4 <br>\n",
    "2 - 1300     4 <br>\n",
    "3 - 1250     2 <br>\n",
    "\n",
    "logically 1 and 2 are the nearest (in terms of Euclidean Distance) but if we dont scale we will get 1 and 3 to be nearest as the larger range value overwhelms the noofbedrooms feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be for __min max scaling__ <br>\n",
    "In this we have for feature __x<sub>i</sub>__  <br>\n",
    "## x<sub>i</sub><sup>new</sup> as (x<sub>i</sub><sup>old</sup>-x<sub>i</sub><sup>min</sup>)/(x<sub>i</sub><sup>max</sup>-x<sub>i</sub><sup>min</sup>)\n",
    "The range will be between 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to subtract is to __subtract ximean from all values to get mean as 0 and rest as +ve or -ve around it__ <br>\n",
    "\n",
    "To change range from (0-1) to some (min-max) , we do <br>\n",
    "\n",
    "__val in (0-1) * (max-min) + (min)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature learning in sklearn\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = np.array([[1,-1,2],[2,0,0],[0,1,-1]], dtype = \"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "[0. 0. 0.]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "x_scaled = preprocessing.scale(x)\n",
    "\n",
    "print(x_scaled)\n",
    "print(x_scaled.mean(axis = 0)) # for row-wise\n",
    "print(x_scaled.std(axis = 0)) #for row wise\n",
    "\n",
    "#see how mean has been made 0 for all rows and standard deviation is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above scale function is that , usually we have a training data and a testing data ... doing scale on test and train data sperately seems wrong since different operations are happening since the mean etc are different for the training and testing data ... hence we should always scale first and then split <br>\n",
    "<br>\n",
    "One case is where the testing data is provided later on , so what we want to do is that scale training data , store the parameters of the training and then use the same parameters on the testing data ... however this is not possible with scale() function , so use StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler() #we have a lot of parameters on this , mean , min, max etc\n",
    "scaler.fit(x) #this will fit the parameters of scaling into scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n"
     ]
    }
   ],
   "source": [
    "#now we can transform various different data based on these parameters using transform function\n",
    "\n",
    "print(scaler.transform(x)) #gives same value as .scale() function as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.22474487 -0.26726124]]\n"
     ]
    }
   ],
   "source": [
    "newdata = np.array([[1,1,0]], dtype=\"float64\")\n",
    "print(scaler.transform(newdata))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
